---
title: null
output: html_document
date: null
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load necessary packages
library(knitr)
library(kableExtra)
```
<h2 style="text-align: center;">Homework 4 Theory</h2>


## Question 1
<b>
Assume $X$ is a discrete random variable that takes values in $\{1,2,3\}$, with probability defined by

$$
\begin{aligned}
& \operatorname{Pr}(X=1)=\theta_{1} \\
& \operatorname{Pr}(X=2)=2 \theta_{1} \\
& \operatorname{Pr}(X=3)=\theta_{2}
\end{aligned}
$$

where $\theta=\left[\theta_{1}, \theta_{2}\right]$ is an unknown parameter to be estimated.

Now assume we observe a sequence $D:=\left\{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\right\}$ that is _independent and identically distributed (i.i.d.)_ from the distribution. We assume the number of observations of the values: $1,2,3$ in $D$ are $s_{1}, s_{2}, s_{3}$, respectively.
</b>

**(a) To ensure that $\operatorname{Pr}(X=i)$ is a valid probability mass function, what constraint should we put on $\theta=\left[\theta_{1}, \theta_{2}\right]$ ? Write your answers quantitatively as expressions that include $\theta_{1}$ and $\theta_{2}$.**


To ensure that \(\operatorname{Pr}(X=i)\) is a valid probability mass function, the probabilities must satisfy two conditions:

1. Each probability must be non-negative.
2. The probabilities must sum to 1.

Given the probabilities:
\[
\begin{aligned}
& \operatorname{Pr}(X=1) = \theta_{1}, \\
& \operatorname{Pr}(X=2) = 2\theta_{1}, \\
& \operatorname{Pr}(X=3) = \theta_{2},
\end{aligned}
\]
we can derive the constraints as follows:

### Non-negativity Condition
Each probability must be non-negative:
\[
\begin{aligned}
& \theta_{1} \geq 0, \\
& 2\theta_{1} \geq 0, \\
& \theta_{2} \geq 0.
\end{aligned}
\]
Since \(\theta_{1} \geq 0\) implies \(2\theta_{1} \geq 0\), the conditions simplify to:
\[
\theta_{1} \geq 0, \quad \theta_{2} \geq 0.
\]

### Sum-to-One Condition
The probabilities must sum to 1:
\[
\theta_{1} + 2\theta_{1} + \theta_{2} = 1.
\]
Simplifying, we get:
\[
3\theta_{1} + \theta_{2} = 1.
\]

### Summary of Constraints
To ensure that \(\operatorname{Pr}(X=i)\) is a valid probability mass function, the parameters \(\theta = [\theta_{1}, \theta_{2}]\) must satisfy the following constraints:
\[
\begin{aligned}
& \theta_{1} \geq 0, \\
& \theta_{2} \geq 0, \\
& 3\theta_{1} + \theta_{2} = 1.
\end{aligned}
\]

These constraints ensure that the probabilities are non-negative and sum to 1, making \(\operatorname{Pr}(X=i)\) a valid probability mass function.


<b>
(b) Write down the joint probability of the data sequence

$$
\operatorname{Pr}(D \mid \theta)=\operatorname{Pr}\left(\left\{x^{(1)}, \ldots, x^{(n)}\right\} \mid \theta\right)
$$

and the $\log$ probability $\log \operatorname{Pr}(D \mid \theta)$.
</b>


To write down the joint probability of the data sequence \( D = \{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\} \) given the parameter \(\theta\), we first use the fact that the observations are independent and identically distributed (i.i.d.). This allows us to express the joint probability as the product of the individual probabilities of each observation.

Given the probabilities:
\[
\operatorname{Pr}(X=1) = \theta_1, \quad \operatorname{Pr}(X=2) = 2\theta_1, \quad \operatorname{Pr}(X=3) = \theta_2,
\]
and the counts \(s_1, s_2, s_3\) of observations of \(1, 2, 3\) respectively, we can write the joint probability \(\operatorname{Pr}(D \mid \theta)\) as follows:

\[
\operatorname{Pr}(D \mid \theta) = \prod_{i=1}^n \operatorname{Pr}(X = x^{(i)} \mid \theta).
\]

Since we have \(s_1\) observations of \(1\), \(s_2\) observations of \(2\), and \(s_3\) observations of \(3\):

\[
\operatorname{Pr}(D \mid \theta) = \theta_1^{s_1} \cdot (2\theta_1)^{s_2} \cdot \theta_2^{s_3}.
\]

Combining these terms, we get:

\[
\operatorname{Pr}(D \mid \theta) = \theta_1^{s_1} \cdot (2\theta_1)^{s_2} \cdot \theta_2^{s_3} = \theta_1^{s_1 + s_2} \cdot 2^{s_2} \cdot \theta_2^{s_3}.
\]

### Log Probability

To find the log probability \(\log \operatorname{Pr}(D \mid \theta)\), we take the natural logarithm of the joint probability:

\[
\log \operatorname{Pr}(D \mid \theta) = \log \left( \theta_1^{s_1 + s_2} \cdot 2^{s_2} \cdot \theta_2^{s_3} \right).
\]

Using the properties of logarithms, we can break this into a sum:

\[
\log \operatorname{Pr}(D \mid \theta) = \log (\theta_1^{s_1 + s_2}) + \log (2^{s_2}) + \log (\theta_2^{s_3}).
\]

This simplifies to:

\[
\log \operatorname{Pr}(D \mid \theta) = (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log \theta_2.
\]

Thus, the log probability is:

\[
\log \operatorname{Pr}(D \mid \theta) = (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log \theta_2.
\]

**(c) Calculate the maximum likelihood estimation $\hat{\theta}$ of $\theta$ based on the sequence D.**


To calculate the maximum likelihood estimation (MLE) \(\hat{\theta}\) of \(\theta\) based on the sequence \(D\), we need to maximize the log likelihood function derived in part (b):

\[
\log \operatorname{Pr}(D \mid \theta) = (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log \theta_2.
\]

Given the constraint from part (a) that \(3\theta_1 + \theta_2 = 1\), we can use this constraint to express \(\theta_2\) in terms of \(\theta_1\):

\[
\theta_2 = 1 - 3\theta_1.
\]

Substitute this into the log likelihood function:

\[
\log \operatorname{Pr}(D \mid \theta) = (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log (1 - 3\theta_1).
\]

To find the MLE, we need to maximize this log likelihood function with respect to \(\theta_1\). To do this, we take the derivative with respect to \(\theta_1\) and set it to zero:

\[
\frac{d}{d\theta_1} \left[ (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log (1 - 3\theta_1) \right] = 0.
\]

Calculate the derivative:

\[
\frac{d}{d\theta_1} \left[ (s_1 + s_2) \log \theta_1 \right] = \frac{s_1 + s_2}{\theta_1},
\]

\[
\frac{d}{d\theta_1} \left[ s_3 \log (1 - 3\theta_1) \right] = -\frac{3s_3}{1 - 3\theta_1}.
\]

Set the derivative to zero:

\[
\frac{s_1 + s_2}{\theta_1} - \frac{3s_3}{1 - 3\theta_1} = 0.
\]

Rearrange to solve for \(\theta_1\):

\[
\frac{s_1 + s_2}{\theta_1} = \frac{3s_3}{1 - 3\theta_1},
\]

\[
(s_1 + s_2)(1 - 3\theta_1) = 3s_3\theta_1,
\]

\[
s_1 + s_2 - 3\theta_1(s_1 + s_2) = 3s_3\theta_1,
\]

\[
s_1 + s_2 = 3\theta_1(s_1 + s_2 + s_3).
\]

Solve for \(\theta_1\):

\[
\theta_1 = \frac{s_1 + s_2}{3(s_1 + s_2 + s_3)}.
\]

Now, using the constraint \(\theta_2 = 1 - 3\theta_1\):

\[
\theta_2 = 1 - 3\left( \frac{s_1 + s_2}{3(s_1 + s_2 + s_3)} \right),
\]

\[
\theta_2 = 1 - \frac{s_1 + s_2}{s_1 + s_2 + s_3},
\]

\[
\theta_2 = \frac{s_3}{s_1 + s_2 + s_3}.
\]

Thus, the maximum likelihood estimators \(\hat{\theta}_1\) and \(\hat{\theta}_2\) are:

\[
\hat{\theta}_1 = \frac{s_1 + s_2}{3(s_1 + s_2 + s_3)},
\]

\[
\hat{\theta}_2 = \frac{s_3}{s_1 + s_2 + s_3}.
\]

## Question 2
<b>
Let $\left\{x^{(1)}, \ldots, x^{(n)}\right\}$ be an _i.i.d_. sample from an exponential distribution, whose the density function is defined as

$$
f(x \mid \beta)=\frac{1}{\beta} \exp \left(-\frac{x}{\beta}\right), \quad \text { for } \quad 0 \leq x<\infty
$$

Please find the maximum likelihood estimator (MLE) of the parameter $\beta$. Show your work.
</b>


To find the maximum likelihood estimator (MLE) of the parameter \(\beta\) for an i.i.d. sample from an exponential distribution, we follow these steps:

### Step 1: Write Down the Likelihood Function

Given the density function of the exponential distribution:

\[
f(x \mid \beta) = \frac{1}{\beta} \exp \left(-\frac{x}{\beta}\right), \quad \text{for } 0 \leq x < \infty,
\]

the likelihood function for the i.i.d. sample \( \{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\} \) is:

\[
L(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \prod_{i=1}^n f(x^{(i)} \mid \beta).
\]

Substitute the density function into the product:

\[
L(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \prod_{i=1}^n \left( \frac{1}{\beta} \exp \left( -\frac{x^{(i)}}{\beta} \right) \right).
\]

### Step 2: Simplify the Likelihood Function

Combine the terms inside the product:

\[
L(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \left( \frac{1}{\beta} \right)^n \exp \left( -\frac{1}{\beta} \sum_{i=1}^n x^{(i)} \right).
\]

### Step 3: Write Down the Log-Likelihood Function

The log-likelihood function is the natural logarithm of the likelihood function:

\[
\ell(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \log L(\beta \mid x^{(1)}, \ldots, x^{(n)}).
\]

Substitute the likelihood function:

\[
\ell(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \log \left( \left( \frac{1}{\beta} \right)^n \exp \left( -\frac{1}{\beta} \sum_{i=1}^n x^{(i)} \right) \right).
\]

Use the properties of logarithms to simplify:

\[
\ell(\beta \mid x^{(1)}, \ldots, x^{(n)}) = n \log \left( \frac{1}{\beta} \right) + \log \left( \exp \left( -\frac{1}{\beta} \sum_{i=1}^n x^{(i)} \right) \right).
\]

\[
\ell(\beta \mid x^{(1)}, \ldots, x^{(n)}) = -n \log \beta - \frac{1}{\beta} \sum_{i=1}^n x^{(i)}.
\]

### Step 4: Differentiate the Log-Likelihood Function

Differentiate the log-likelihood function with respect to \(\beta\):

\[
\frac{\partial \ell(\beta \mid x^{(1)}, \ldots, x^{(n)})}{\partial \beta} = -n \frac{1}{\beta} + \frac{1}{\beta^2} \sum_{i=1}^n x^{(i)}.
\]

### Step 5: Set the Derivative Equal to Zero

Set the derivative equal to zero to find the critical points:

\[
-n \frac{1}{\beta} + \frac{1}{\beta^2} \sum_{i=1}^n x^{(i)} = 0.
\]

Multiply through by \(\beta^2\):

\[
-n\beta + \sum_{i=1}^n x^{(i)} = 0.
\]

Solve for \(\beta\):

\[
n\beta = \sum_{i=1}^n x^{(i)},
\]

\[
\beta = \frac{1}{n} \sum_{i=1}^n x^{(i)}.
\]

### Step 6: Verify the Critical Point is a Maximum

To verify that this critical point corresponds to a maximum, we can check the second derivative of the log-likelihood function. The second derivative is:

\[
\frac{\partial^2 \ell(\beta \mid x^{(1)}, \ldots, x^{(n)})}{\partial \beta^2} = n \frac{1}{\beta^2} - 2 \frac{1}{\beta^3} \sum_{i=1}^n x^{(i)}.
\]

Substitute \(\beta = \frac{1}{n} \sum_{i=1}^n x^{(i)}\) into the second derivative:

\[
\frac{\partial^2 \ell(\beta \mid x^{(1)}, \ldots, x^{(n)})}{\partial \beta^2} = n \frac{1}{\left(\frac{1}{n} \sum_{i=1}^n x^{(i)}\right)^2} - 2 \frac{1}{\left(\frac{1}{n} \sum_{i=1}^n x^{(i)}\right)^3} \sum_{i=1}^n x^{(i)}.
\]

Simplifying, we see that this expression is negative, confirming that the critical point is a maximum.

### Conclusion

The maximum likelihood estimator (MLE) of \(\beta\) is:

\[
\hat{\beta} = \frac{1}{n} \sum_{i=1}^n x^{(i)} = \bar{x}.
\]

So the MLE of \(\beta\) is the sample mean of the observed data.


## Question 3
**(a) Assume that you want to investigate the proportion ( $\theta$ ) of defective items manufactured at a production line. You take a random sample of 30 items and found 5 of them were defective. Assume the prior of $\theta$ is a uniform distribution on $[0,1]$. Please compute the posterior of $\theta$. It is sufficient to write down the posterior density function upto a normalization constant that does not depend on $\theta$.**


To compute the posterior distribution of \(\theta\) given the data, we use Bayes' theorem. Let's denote the number of defective items by \(D\) and the total number of items sampled by \(N\).

Given:
- \(N = 30\)
- \(D = 5\)
- Prior distribution: \(\theta \sim \text{Uniform}(0, 1)\)

The likelihood function for the data given \(\theta\) is based on the binomial distribution:

\[
\operatorname{Pr}(D = d \mid \theta) = \binom{N}{D} \theta^D (1 - \theta)^{N-D}.
\]

The prior distribution is uniform on \([0, 1]\):

\[
\pi(\theta) = 1, \quad \text{for } \theta \in [0, 1].
\]

By Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:

\[
\pi(\theta \mid D) \propto \operatorname{Pr}(D \mid \theta) \pi(\theta).
\]

Substitute the likelihood and prior:

\[
\pi(\theta \mid D) \propto \binom{30}{5} \theta^5 (1 - \theta)^{25} \cdot 1.
\]

Since the binomial coefficient \(\binom{30}{5}\) is a constant that does not depend on \(\theta\), we can write the posterior density function up to a normalization constant as:

\[
\pi(\theta \mid D) \propto \theta^5 (1 - \theta)^{25}.
\]

Thus, the posterior density function, up to a normalization constant, is:

\[
\pi(\theta \mid D) \propto \theta^5 (1 - \theta)^{25}.
\]

This is the unnormalized posterior density function for \(\theta\). The normalization constant can be found by integrating this density over the interval \([0, 1]\) and ensuring the total probability integrates to 1, but it is not required for this problem.

***(b) Assume an observation $D:=\left\{x^{(1)}, \ldots, x^{(n)}\right\}$ is _i.i.d._ drawn from a Gaussian distribution $\mathcal{N}(\mu, 1)$, with an unknown mean $\mu$ and a variance of 1 . Assume the prior distribution of $\mu$ is $\mathcal{N}(0,1)$. Please derive the posterior distribution $p(\mu \mid D)$ of $\mu$ given data $D$.**


To derive the posterior distribution \( p(\mu \mid D) \) for the mean \(\mu\) of a Gaussian distribution \(\mathcal{N}(\mu, 1)\) given data \( D = \{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\} \) and a prior distribution \(\mu \sim \mathcal{N}(0, 1)\), we use Bayes' theorem.

### Step 1: Write Down the Likelihood Function

The likelihood function for \( n \) i.i.d. observations from \(\mathcal{N}(\mu, 1)\) is:

\[
p(D \mid \mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(x^{(i)} - \mu)^2}{2}\right).
\]

Simplifying the product, we get:

\[
p(D \mid \mu) = (2\pi)^{-n/2} \exp\left(-\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2}\right).
\]

### Step 2: Write Down the Prior Distribution

The prior distribution is given by \(\mu \sim \mathcal{N}(0, 1)\):

\[
p(\mu) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{\mu^2}{2}\right).
\]

### Step 3: Combine the Likelihood and Prior Using Bayes' Theorem

The posterior distribution is proportional to the product of the likelihood and the prior:

\[
p(\mu \mid D) \propto p(D \mid \mu) p(\mu).
\]

Substitute the likelihood and prior:

\[
p(\mu \mid D) \propto (2\pi)^{-n/2} \exp\left(-\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2}\right) \cdot \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{\mu^2}{2}\right).
\]

Combine the exponentials:

\[
p(\mu \mid D) \propto (2\pi)^{-(n+1)/2} \exp\left(-\left(\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2} + \frac{\mu^2}{2}\right)\right).
\]

Simplify the expression inside the exponential:

\[
\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2} + \frac{\mu^2}{2} = \frac{1}{2} \left( \sum_{i=1}^n (x^{(i)} - \mu)^2 + \mu^2 \right).
\]

Expanding \(\sum_{i=1}^n (x^{(i)} - \mu)^2\):

\[
\sum_{i=1}^n (x^{(i)} - \mu)^2 = \sum_{i=1}^n (x^{(i)})^2 - 2\mu \sum_{i=1}^n x^{(i)} + n\mu^2.
\]

Combining terms:

\[
\frac{1}{2} \left( \sum_{i=1}^n (x^{(i)})^2 - 2\mu \sum_{i=1}^n x^{(i)} + n\mu^2 + \mu^2 \right) = \frac{1}{2} \left( \sum_{i=1}^n (x^{(i)})^2 - 2\mu \sum_{i=1}^n x^{(i)} + (n + 1)\mu^2 \right).
\]

### Step 4: Recognize the Posterior Distribution

We can see that the exponent of the posterior distribution has the form of a quadratic in \(\mu\):

\[
-\frac{1}{2} \left( (n + 1)\mu^2 - 2\mu \sum_{i=1}^n x^{(i)} + \sum_{i=1}^n (x^{(i)})^2 \right).
\]

Completing the square for \(\mu\):

\[
(n + 1)\mu^2 - 2\mu \sum_{i=1}^n x^{(i)} = (n + 1) \left( \mu^2 - \frac{2\mu \sum_{i=1}^n x^{(i)}}{n + 1} \right) = (n + 1) \left( \mu - \frac{\sum_{i=1}^n x^{(i)}}{n + 1} \right)^2 - \frac{\left( \sum_{i=1}^n x^{(i)} \right)^2}{n + 1}.
\]

So the exponent becomes:

\[
-\frac{1}{2} \left( (n + 1) \left( \mu - \frac{\sum_{i=1}^n x^{(i)}}{n + 1} \right)^2 - \frac{\left( \sum_{i=1}^n x^{(i)} \right)^2}{n + 1} + \sum_{i=1}^n (x^{(i)})^2 \right).
\]

We can see that this has the form of the exponent of a normal distribution. Therefore, the posterior distribution of \(\mu\) given the data \(D\) is a normal distribution. The mean of this normal distribution is:

\[
\mu_{posterior} = \frac{\sum_{i=1}^n x^{(i)}}{n + 1}.
\]

The variance is the inverse of the coefficient of the quadratic term in \(\mu\):

\[
\sigma^2_{posterior} = \frac{1}{n + 1}.
\]

### Conclusion

The posterior distribution \( p(\mu \mid D) \) is:

\[
p(\mu \mid D) = \mathcal{N}\left(\frac{\sum_{i=1}^n x^{(i)}}{n + 1}, \frac{1}{n + 1}\right).
\]