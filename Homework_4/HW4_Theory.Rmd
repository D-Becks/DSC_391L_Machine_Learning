---
title: null
output: html_document
date: null
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load necessary packages
library(knitr)
library(kableExtra)
```
<h2 style="text-align: center;">Homework 4 Theory</h2>


## Question 1
<b>
Assume $X$ is a discrete random variable that takes values in $\{1,2,3\}$, with probability defined by

$$
\begin{aligned}
& \operatorname{Pr}(X=1)=\theta_{1} \\
& \operatorname{Pr}(X=2)=2 \theta_{1} \\
& \operatorname{Pr}(X=3)=\theta_{2}
\end{aligned}
$$

where $\theta=\left[\theta_{1}, \theta_{2}\right]$ is an unknown parameter to be estimated.

Now assume we observe a sequence $D:=\left\{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\right\}$ that is _independent and identically distributed (i.i.d.)_ from the distribution. We assume the number of observations of the values: $1,2,3$ in $D$ are $s_{1}, s_{2}, s_{3}$, respectively.
</b>

**(a) To ensure that $\operatorname{Pr}(X=i)$ is a valid probability mass function, what constraint should we put on $\theta=\left[\theta_{1}, \theta_{2}\right]$ ? Write your answers quantitatively as expressions that include $\theta_{1}$ and $\theta_{2}$.**


As these proabilities make up a complete pmf, we know that each probability is between $0$ and $1$ and that the sum of them is equal to $1$. 
$$
\theta_{1} + 2\theta_{1} + \theta_{2} = 1.
$$
Which can be simplified to:
$$
3\theta_{1} + \theta_{2} = 1.
$$

As mentioned above the constraints are that these probabilities are the only probabilities that make up the pmf and that they are non-negative and their sum is exactly equal to $1$.


<b>
(b) Write down the joint probability of the data sequence

$$
\operatorname{Pr}(D \mid \theta)=\operatorname{Pr}\left(\left\{x^{(1)}, \ldots, x^{(n)}\right\} \mid \theta\right)
$$

and the $\log$ probability $\log \operatorname{Pr}(D \mid \theta)$.
</b>

Given the probabilities above as well as the counts $s_1, s_2, s_3$ respectively, the joint probability is below:

$$
\operatorname{Pr}(D \mid \theta) = \prod_{i=1}^n \operatorname{Pr}(X = x^{(i)} \mid \theta).
$$

Since we know the count of observations for each respective probability, we can expand it as follows:
$$
\operatorname{Pr}(D \mid \theta) = \theta_1^{s_1} \cdot (2\theta_1)^{s_2} \cdot \theta_2^{s_3}.
$$

To find the log probability, we can first combine terms to separate off the $\theta$s and then apply the log of each side:

$$
\log \operatorname{Pr}(D \mid \theta) = \log \left( \theta_1^{s_1 + s_2} \cdot 2^{s_2} \cdot \theta_2^{s_3} \right).
$$

Which simplifies to:

$$
\log \operatorname{Pr}(D \mid \theta) = (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log \theta_2.
$$

**(c) Calculate the maximum likelihood estimation $\hat{\theta}$ of $\theta$ based on the sequence D.**


First in order to reduce the number of varibales we will simplify the expression by using $\theta_2$ in terms of $\theta_1$ by using the fact that the probabilities sum to $1$:

$$
\theta_2 = 1 - 3\theta_1.
$$

Substitute this into our previous expression in part b:

$$
\log \operatorname{Pr}(D \mid \theta) = (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log (1 - 3\theta_1).
$$

Now we will maximize this function with respect to $\theta_1$ by taking the derivative and setting it equal to $0$:

$$
\frac{d}{d\theta_1} \left[ (s_1 + s_2) \log \theta_1 + s_2 \log 2 + s_3 \log (1 - 3\theta_1) \right] = 0.
$$
Which becomes:
$$
\frac{s_1 + s_2}{\theta_1} - \frac{3s_3}{1 - 3\theta_1} = 0.
$$

Solving for $\theta_1$ we get:

$$
\theta_1 = \frac{s_1 + s_2}{3(s_1 + s_2 + s_3)}.
$$

Now we solve for $\theta_2$ by using the substitution expression we used earlier to get:

$$
\theta_2 = 1 - 3\left( \frac{s_1 + s_2}{3(s_1 + s_2 + s_3)} \right),
$$
Which becomes:
$$
\theta_2 = \frac{s_3}{s_1 + s_2 + s_3}.
$$

Thus, the MLE is equal to $\theta_1$ and $\theta_2$ that are solved for above.

## Question 2
<b>
Let $\left\{x^{(1)}, \ldots, x^{(n)}\right\}$ be an _i.i.d_. sample from an exponential distribution, whose the density function is defined as

$$
f(x \mid \beta)=\frac{1}{\beta} \exp \left(-\frac{x}{\beta}\right), \quad \text { for } \quad 0 \leq x<\infty
$$

Please find the maximum likelihood estimator (MLE) of the parameter $\beta$. Show your work.
</b>


To solve for this we will do the exact same process that we did above for $\theta$ but instead solve for the MLE of $\beta$.

First we define the likelihood function from the density function given above:
$$
L(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \prod_{i=1}^n \left( \frac{1}{\beta} \exp \left( -\frac{x^{(i)}}{\beta} \right) \right).
$$
Now we apply the log to the function in order to find the log likliehood function:
$$
\ell(\beta \mid x^{(1)}, \ldots, x^{(n)}) = \log \left( \left( \frac{1}{\beta} \right)^n \exp \left( -\frac{1}{\beta} \sum_{i=1}^n x^{(i)} \right) \right).
$$

Which simplifies to:
$$
\ell(\beta \mid x^{(1)}, \ldots, x^{(n)}) = -n \log \beta - \frac{1}{\beta} \sum_{i=1}^n x^{(i)}.
$$

Now we find the derivative, by using the fact that if $f(x) = log(x)$ then $f'(x) = \frac{1}{x}$,  and set it equal to $0$:

$$
\frac{\partial \ell(\beta \mid x^{(1)}, \ldots, x^{(n)})}{\partial \beta} = -n \frac{1}{\beta} + \frac{1}{\beta^2} \sum_{i=1}^n x^{(i)} = 0
$$

We will simplify the expression by multiply by $\beta^2$ and then solving for $\beta$:
$$
-n \beta + \sum_{i=1}^n x^{(i)} = 0
$$

$$
\beta = \frac{1}{n} \sum_{i=1}^n x^{(i)} = \bar{x}
$$
So the MLE of $\beta$ is simplified to equal the sample mean of the dataset, which makes sense given the density function provided.


## Question 3
**(a) Assume that you want to investigate the proportion ( $\theta$ ) of defective items manufactured at a production line. You take a random sample of 30 items and found 5 of them were defective. Assume the prior of $\theta$ is a uniform distribution on $[0,1]$. Please compute the posterior of $\theta$. It is sufficient to write down the posterior density function upto a normalization constant that does not depend on $\theta$.**


To compute the posterior distribution of \(\theta\) given the data, we use Bayes' theorem. Let's denote the number of defective items by \(D\) and the total number of items sampled by \(N\).

Given:
- \(N = 30\)
- \(D = 5\)
- Prior distribution: \(\theta \sim \text{Uniform}(0, 1)\)

The likelihood function for the data given \(\theta\) is based on the binomial distribution:

\[
\operatorname{Pr}(D = d \mid \theta) = \binom{N}{D} \theta^D (1 - \theta)^{N-D}.
\]

The prior distribution is uniform on \([0, 1]\):

\[
\pi(\theta) = 1, \quad \text{for } \theta \in [0, 1].
\]

By Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:

\[
\pi(\theta \mid D) \propto \operatorname{Pr}(D \mid \theta) \pi(\theta).
\]

Substitute the likelihood and prior:

\[
\pi(\theta \mid D) \propto \binom{30}{5} \theta^5 (1 - \theta)^{25} \cdot 1.
\]

Since the binomial coefficient \(\binom{30}{5}\) is a constant that does not depend on \(\theta\), we can write the posterior density function up to a normalization constant as:

\[
\pi(\theta \mid D) \propto \theta^5 (1 - \theta)^{25}.
\]

Thus, the posterior density function, up to a normalization constant, is:

\[
\pi(\theta \mid D) \propto \theta^5 (1 - \theta)^{25}.
\]

This is the unnormalized posterior density function for \(\theta\). The normalization constant can be found by integrating this density over the interval \([0, 1]\) and ensuring the total probability integrates to 1, but it is not required for this problem.

***(b) Assume an observation $D:=\left\{x^{(1)}, \ldots, x^{(n)}\right\}$ is _i.i.d._ drawn from a Gaussian distribution $\mathcal{N}(\mu, 1)$, with an unknown mean $\mu$ and a variance of 1 . Assume the prior distribution of $\mu$ is $\mathcal{N}(0,1)$. Please derive the posterior distribution $p(\mu \mid D)$ of $\mu$ given data $D$.**


To derive the posterior distribution \( p(\mu \mid D) \) for the mean \(\mu\) of a Gaussian distribution \(\mathcal{N}(\mu, 1)\) given data \( D = \{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\} \) and a prior distribution \(\mu \sim \mathcal{N}(0, 1)\), we use Bayes' theorem.

### Step 1: Write Down the Likelihood Function

The likelihood function for \( n \) i.i.d. observations from \(\mathcal{N}(\mu, 1)\) is:

\[
p(D \mid \mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(x^{(i)} - \mu)^2}{2}\right).
\]

Simplifying the product, we get:

\[
p(D \mid \mu) = (2\pi)^{-n/2} \exp\left(-\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2}\right).
\]

### Step 2: Write Down the Prior Distribution

The prior distribution is given by \(\mu \sim \mathcal{N}(0, 1)\):

\[
p(\mu) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{\mu^2}{2}\right).
\]

### Step 3: Combine the Likelihood and Prior Using Bayes' Theorem

The posterior distribution is proportional to the product of the likelihood and the prior:

\[
p(\mu \mid D) \propto p(D \mid \mu) p(\mu).
\]

Substitute the likelihood and prior:

\[
p(\mu \mid D) \propto (2\pi)^{-n/2} \exp\left(-\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2}\right) \cdot \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{\mu^2}{2}\right).
\]

Combine the exponentials:

\[
p(\mu \mid D) \propto (2\pi)^{-(n+1)/2} \exp\left(-\left(\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2} + \frac{\mu^2}{2}\right)\right).
\]

Simplify the expression inside the exponential:

\[
\sum_{i=1}^n \frac{(x^{(i)} - \mu)^2}{2} + \frac{\mu^2}{2} = \frac{1}{2} \left( \sum_{i=1}^n (x^{(i)} - \mu)^2 + \mu^2 \right).
\]

Expanding \(\sum_{i=1}^n (x^{(i)} - \mu)^2\):

\[
\sum_{i=1}^n (x^{(i)} - \mu)^2 = \sum_{i=1}^n (x^{(i)})^2 - 2\mu \sum_{i=1}^n x^{(i)} + n\mu^2.
\]

Combining terms:

\[
\frac{1}{2} \left( \sum_{i=1}^n (x^{(i)})^2 - 2\mu \sum_{i=1}^n x^{(i)} + n\mu^2 + \mu^2 \right) = \frac{1}{2} \left( \sum_{i=1}^n (x^{(i)})^2 - 2\mu \sum_{i=1}^n x^{(i)} + (n + 1)\mu^2 \right).
\]

### Step 4: Recognize the Posterior Distribution

We can see that the exponent of the posterior distribution has the form of a quadratic in \(\mu\):

\[
-\frac{1}{2} \left( (n + 1)\mu^2 - 2\mu \sum_{i=1}^n x^{(i)} + \sum_{i=1}^n (x^{(i)})^2 \right).
\]

Completing the square for \(\mu\):

\[
(n + 1)\mu^2 - 2\mu \sum_{i=1}^n x^{(i)} = (n + 1) \left( \mu^2 - \frac{2\mu \sum_{i=1}^n x^{(i)}}{n + 1} \right) = (n + 1) \left( \mu - \frac{\sum_{i=1}^n x^{(i)}}{n + 1} \right)^2 - \frac{\left( \sum_{i=1}^n x^{(i)} \right)^2}{n + 1}.
\]

So the exponent becomes:

\[
-\frac{1}{2} \left( (n + 1) \left( \mu - \frac{\sum_{i=1}^n x^{(i)}}{n + 1} \right)^2 - \frac{\left( \sum_{i=1}^n x^{(i)} \right)^2}{n + 1} + \sum_{i=1}^n (x^{(i)})^2 \right).
\]

We can see that this has the form of the exponent of a normal distribution. Therefore, the posterior distribution of \(\mu\) given the data \(D\) is a normal distribution. The mean of this normal distribution is:

\[
\mu_{posterior} = \frac{\sum_{i=1}^n x^{(i)}}{n + 1}.
\]

The variance is the inverse of the coefficient of the quadratic term in \(\mu\):

\[
\sigma^2_{posterior} = \frac{1}{n + 1}.
\]

### Conclusion

The posterior distribution \( p(\mu \mid D) \) is:

\[
p(\mu \mid D) = \mathcal{N}\left(\frac{\sum_{i=1}^n x^{(i)}}{n + 1}, \frac{1}{n + 1}\right).
\]